\documentclass[journal,twoside]{IEEEtran}

%% Citations
\usepackage{cite}
%\usepackage[sort&compress]{natbib}
%\bibpunct{[}{]}{,}{n}{,}{,}

%% AMS
\usepackage[cmex10]{amsmath}
\usepackage{amssymb,amsfonts}
\interdisplaylinepenalty=2500

%% Balance columns
%\usepackage{balance}

%% Better arrays
\usepackage{array}

%% Enumeration
\usepackage{enumerate}

%% Figures
\usepackage[pdftex]{graphicx}
\graphicspath{/Figures/}
\DeclareGraphicsExtensions{.png}
\usepackage[section]{placeins}

%% Fix some formatting issues
\usepackage{fixltx2e}
\usepackage{dblfloatfix}

%% Better cross-referencing
\usepackage[capitalize]{cleveref}

%% Newline for paragraphs instead of indenting
%\setlength{\parindent}{0in}
%\setlength{\parskip}{5pt plus 1pt minus 1pt}

\usepackage{threeparttable} % More control over tables

\usepackage{xcolor} % Use this for different colors -- useful for indicating what's changed since last review round
\newcommand{\changed}[1]{\textcolor{red}{#1}}
%\renewcommand{\changed}[1]{#1} %Uncomment this to get rid of the red stuff

%% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% The paper headers
\markboth{IEEE Transactions on Components, Packaging, and Manufacturing Technology, Vol. ??, No. ??, Month ??, 20??}{Wahby \MakeLowercase{\textit{et al.}}: A Virtual Integration Platform for 2D and 3D IC Design Space Exploration}


\begin{document}
%% fix dashed lines for repeated author names in IEEE style bibliographies
%% NOTE: This requires some fiddling with the bibliography as well
\bstctlcite{IEEEexample:BSTcontrol} 

%% paper title
%% can use linebreaks \\ within to get better formatting as desired
\title{\changed{Compact Modeling of Trends in 2D and 3DIC Power, Signal, and Thermal Performance}}
\author{William~Wahby,~\IEEEmembership{Student~Member,~IEEE}, Li~Zheng, Yang~Zhang,~\IEEEmembership{Student~Member,~IEEE}, and Muhannad~Bakir,~\IEEEmembership{Senior~Member,~IEEE}%
	\thanks{Original manuscript received July 11, 2015. \changed{Revised manuscript received October ??, 2015.} This work was supported in part by the SRC GRC project 2254.001, and by the SRC Educational Alliance Intel Foundation Fellowship}%
\thanks{William Wahby, Li Zheng, Yang Zhang, and Muhannad Bakir are with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332 USA. (emails: wwahby3@gatech.edu; lizheng@gatech.edu; steven.zhang@gatech.edu; muhannad.bakir@mirc.gatech.edu)}}




\maketitle

%\IEEEpeerreviewmaketitle


%% =================================
%% ========== ABSTRACT =============
%% =================================
\begin{abstract}
In order to compare the costs and benefits of 2D and 3DIC technologies, an compact simulation framework for 3DIC system evaluation and
design space exploration is developed. The simulator is implemented in MATLAB, and is composed of several
simulation modules, including a compact 3DIC wire length distribution,
a wire pitch and repeater insertion module, a 2D and 3DIC power supply noise
estimation module, and a finite difference thermal simulator.
The simulator is validated against published data for several commercial 2D processors at the 65nm, 45nm, and 32nm nodes.
In order to quantify the benefits of both 2D and 3D integration approaches, a 32nm CPU core is modeled and the impact of several
technology parameters, including interlayer dielectric (ILD) material, on-chip wire material, die thickness, 
and cooling solution are explored. 3D integration is shown to provide a significant power reduction for the 32nm test case,
but more aggressive cooling solutions must be employed to maintain the same clock frequency, due to the increased
areal power density of the 3D CPU.
\end{abstract}

\begin{IEEEkeywords}
Three-dimensional integrated circuits, integrated circuit modeling, integrated circuit interconnections, power dissipation.
\end{IEEEkeywords}

%% =================================
%% ======== INTRODUCTION ===========
%% =================================
\section{Introduction}
\IEEEPARstart{E}{conomic} and physical challenges to conventional 2D scaling are driving interest in 3D integration,
but uncertainty regarding the fabrication costs and system-level tradeoffs of 3D integration
complicate 3DIC design. Projections of 3DIC cost and performance are further complicated by the strongly-coupled
nature of communication, power delivery, and thermal management in 3DICs. Additionally, the 3DIC
design space is complex, as 3DIC design encompasses a broad spectrum of possible design choices and integration
methodologies, ranging from 2.5D interposer-based integration all the way to finely-grained monolithic 
3DICs, as shown in \cref{f-3d-spectrum}, each with unique costs and strengths.

\begin{figure}[!tb]
	\centering
	\includegraphics[width=3.5in]{Figures/spectrum_of_3d_4.png}
	\caption{There are many potential configurations for 3DICs, each with their own costs and advantages.
			Designers must manage the complexity of the 3DIC design space in order to achieve higher performance
			and lower cost systems.}
	\label{f-3d-spectrum}
\end{figure}

Additionally, different technologies must be evaluated for use in both 2D and 3DICs.
Low-k dielectrics can be used to reduce the parasitic capacitances in the wiring stack,
simultaneously improving RC delay and reducing the power consumption of the wiring network.
Alternate wiring materials are also being considered to improve the RC delay of on-chip interconnects,
as well as to reduce the risk of electromigration failures. Fluidic cooling can be used to mitigate the thermal challenges
in high performance 3DICs. In order to understand when a 3D system
might have advantages over a 2D system, all of these factors must be modeled simultaneously.
\changed{Performing these coupled simulations with high fidelity is computationally intensive, however, rendering
thorough exploration of the 3DIC design space challenging.}

In order to investigate the tradeoffs inherent in 3DIC design, we present a compact simulation platform for
rapid exploration of the 3DIC design space. \changed{The aim of this work is the creation of a compact simulation tool enabling
the rapid investigation of what-if questions, in order to understand the trends in performance and power consumption
as materials, devices, and integration methodologies are changed. The compact modeling platform presented here can
then be used to guide more precise investigations into the details of 3DIC physical design.}

The simulation platform consists of several simulation
modules specially tailored for 3DICs, described in \cref{s-virtual-platform}.
\changed{The details of the on-chip wires, TSVs, power delivery network, and steady-state thermal profile
are all modeled in order to develop a complete picture of the overall 3D system performance.}
The simulation platform can also be applied to 2D ICs, allowing for direct comparison of 3D integration
and other more conventional technological improvements.

\changed{This paper first presents the specifics of the models used, and then segues into validation and predictions.}
In \cref{s-virtual-platform} the details of the models used and their interactions are presented.
\changed{In \cref{s-validation} the simulation framework is benchmarked against published
wirelength and TDP data for several commercial processors.}
The simulation tool is used to investigate the impacts of advanced
technologies and integration methodologies on a 32nm Sandy Bridge i7 processing core in \cref{s-2d-investigations,s-3d-investigations}.
Specifically, the impacts of interlayer dielectric material, wire material, and 2D vs 3D integration on the power consumption,
power supply noise, and number of metal layers required for routing are considered.

\section{Simulation Framework} \label{s-virtual-platform}
\changed{The source code for the integrated simulation framework is available online at www.ece.gatech.edu/research/labs/i3ds}.
The simulation platform consists of the following:
 \begin{enumerate}
	\item 3D wire length distribution which accounts for TSV area
	\item Metal layer pitch determination algorithms capable of handling alternate wiring materials
	\item An optimal repeater insertion scheme
	\item Power supply noise models which account for power delivery in 3DICs
	\item  A finite difference thermal module for analyzing the thermal impacts of 3D integration
\end{enumerate}

The overall execution flow is shown in \cref{f-vp-flowchart}.
%\changed{The user first inputs high-level design parameters, including the target clock frequency,
%estimated logic activity factor (the fraction of transistors switching on any given clock cycle), transistor size and
%leakage parameters, wiring material, and expected die area and thickness.
%For 3D designs, the user must also specify the desired TSV diameter and aspect ratio, as well as the number of tiers
%used to implement the 3DIC.}
\changed{The interconnect module first estimates the distribution of 
wire lengths in the system, which is in turn used to determine the number of wiring tiers required for signal routing, 
the wire pitch on each tier, and the number of repeaters needed to meet the delay constraint.}
\changed{
The logic block under consideration is treated as a homogeneous design.
This assumption is commonly used with stochastic wirelength models and allows the rapid estimation of the on-chip 
wire parameters without a priori knowledge about the design specifics, at the cost of the 
loss of insight into fine-grained design details \cite{davis_stochastic_1998,joyner_impact_2001,sekar_intsim_2007}.
}

\changed{
In order to simulate complex heterogeneous designs, the user can input the expected parameters and position of each block
in the system. In this case each block is modeled separately, 
and the results are assembled into an overall power density map of each tier in the design
to determine the thermal profile throughout the stack. In order to better predict the
performance of such systems, the method of \cite{zarkesh-ha_pin_1998} can be used to homogenize
a heterogeneous system. Inter-block connections in heterogeneous designs are ignored, 
as additional design-specific information is needed to determine their properties.
Similarly, off-chip interconnects are ignored when simulating both homogeneous
and heterogeneous designs, as additional design details would be needed for accurate simulation.
}

\begin{figure}[tb]
	\centering
	\includegraphics[width=2.75in]{Figures/vp-flowchart-4.png}
	\caption{Block diagram of the simulation platform execution flow.}
	\label{f-vp-flowchart}
\end{figure}

\changed{Once the on-chip interconnect parameters are known, the on-chip interconnect power consumption can be estimated.
The transistor dynamic and leakage power is also calculated to determine the overall power requirements for the design.
Once the total power consumption is known, it is used in conjunction with the TSV resistance and inductance
and the package pin resistance and inductance
to estimate the simultaneous switching noise in the power delivery network. Additional TSVs (for 3D designs)
or power/ground pads (for 2D designs) are inserted until the simultaneous switching noise drops to acceptable levels.
At this point the tool checks that the total area allocated to TSVs does not exceed a user-specified limit; 
if TSV demand outstrips available area, the TSV diameter is reduced and the interconnect and power modules are rerun.}

\changed{Once the design passes the TSV area check, the thermal module is used to estimate the maximum temperature in the 2D
or 3D design. In order to do this, the material parameters of the die, wiring tiers (including wires and interlayer
dielectric), TSVs, and interstitial layers are input into a finite difference thermal simulator, along with a heat transfer
boundary condition representing the heatsink. If the maximum temperature in the stack
exceeds a user-defined limit, the clock frequency is reduced and the previous modules are rerun until
the maximum temperature is sufficiently reduced. As alternate means of power reduction, the logic activity factor can also be reduced
or the wire or interlayer dielectric materials be modified.
The final design parameters are reported once the constraints on temperature and TSV area are satisfied. }


%% =================================
%% ========= Interconnects ============
%% =================================
\subsection{Interconnect modeling} \label{s-interconnect-modeling}
\changed{The on-chip interconnect network accounts for a significant fraction of the overall
power consumption in many logic designs \cite{sekar_intsim_2007}; accordingly, accurate determination of
the parameters of the on-chip interconnect network is crucial for developing reasonable estimates of
the overall power consumption and operation frequency of an integrated circuit. Specifically,
the number of metal layers used for wiring must be estimated, as well as the pitch of the wires on each level,
in order to determine the overall wiring capacitance, which is a key factor in the maximum wire delay and power consumption.}

Stochastic wire length models have been shown to be effective tools for the rapid prediction
of interconnect properties in 2DICs
\cite{davis_stochastic_1998,sekar_intsim_2007}.
and 3DICs
\cite{joyner_three-dimensional_2000},
but the impact of TSV-induced gate-blockage in 3DICs was not considered until \cite{kim_through-silicon-via_2009-1}
introduced a correction to account for finite TSV size. The method of \cite{kim_through-silicon-via_2009-1}
requires brute-force calculation, however, which is too computationally intensive for rapid simulation. 
To address this issue, a compact correction to the 
wire length distribution was introduced in \cite{wahby_wld_2013}, which is used in this work to
estimate the distribution of wire lengths in 2D and 3D ICs.
Once the wire length distribution is known, the wire pitch and number of metal layers are determined using a bottom-up wire scaling 
technique \cite{venkatesan_performance_1999}, and a delay-optimal repeater insertion scheme 
is used to determine the size and number of repeaters required to meet timing constraints \cite{bakoglu_optimal_1985}.

The impact of surface scattering and grain boundary scattering on wire resistivity are incorporated into the wire sizing algorithm
with a combined Mayadas-Shatzke and Fuchs-Sondheimer (MS+FS) model, with
specularity of $0.55$ and backscattering probability of $0.43$ \cite{sun_surface_2010}.
The metal grain size is approximated as the smallest dimension of each wire.


%% =================================
%% ========= POWER ============
%% =================================

\subsection{Power supply noise modeling}
Power supply noise must be suppressed to ensure reliable system operation, but power delivery in 3DICs is 
complicated by the limited area available for routing power interconnects between tiers.
We use the 3DIC power supply network models developed in \cite{zheng_novel_2014}
to determine the power supply noise in the 3D stack and the number of power delivery TSVs.

%% =================================
%% ========= THERMAL ============
%% =================================
\subsection{Thermal modeling}
Thermal issues are one of the greatest challenges in 3DIC design.
In order to design a thermally robust 3D system, the relationships between device technology, system performance
area constraints, and packaging materials and technology must be explored.
To that end, we utilize a fast and accurate finite difference thermal model, \changed{described in \cite{xie_electrical-thermal_2011},
with a non-conformal meshing strategy described in \cite{zhang-thermal-2014}. If a mesh has multiple materials, 
we assign the volume-weighted thermal conductivity of the surrounding volume to that point.}

\begin{figure}[tb]
	\centering
	\includegraphics[width=1.75in]{Figures/thermal_config_no_bc_4.png}
	\caption{
		\changed{The geometry used in the virtual platform thermal module. TSVs are individually meshed, in order to better capture the impact
		of 3D heat transport.}
	}
	\label{f-thermal-config}
\end{figure}

\changed{The thermal configuration considered in the virtual platform is shown in \cref{f-thermal-config}. One or more dice are assumed
to be stacked vertically atop an interposer (which may be either a conventional organic package substrate, or a silicon or glass
interposer). Each die is separated into three regions: 1) the bulk silicon, 2) the BEOL, which is modeled as an effective material composed of the
volume-weighted average of the thermal conductivities of the wiring material and the interlayer dielectric, and 3) the underfill material
between each die. The power dissipated by each die is applied as an excitation at the boundary between the bulk silicon and the BEOL. 
TSVs are modeled within the bulk of any dice below the top die in the stack. Each external boundary of the stack is
modeled with a convective boundary condition; boundaries internal to the package are typically given a low heat transfer coefficient
of $5$ mW/m$^2$K, while the top and bottom surfaces are given higher values to reflect the cooling method under consideration.}

\changed{The accuracy of this finite difference module was assessed in \cite{zhang-thermal-2014},} in which the performance of the finite difference scheme
was compared against finite element ANSYS models of the same structure. The finite difference model was found to match the ANSYS results with a maximum
error of 2.7\%.




%% =================================
%% ========= VALIDATION ============
%% =================================

\section{Validation} \label{s-validation}
The virtual platform was validated by comparing its predictions against published data for Intel processors ranging from the 65nm node
to the 32nm node
\cite{bai_65nm_process_2004,sakran_65nm_merom_arch_2007,mistry_45nm_process_2007,varghese_45nm_penryn_arch_2007,kumar_45nm_nehalem_arch_2009,packan_32nm_process_2009,kurd_32nm_westmere_arch_2010,yuffe_32nm_sandy_arch_2011}.
For each test case, the chip area, number of logic transistors, number of memory transistors, and size and shape
of the cores and memory blocks were gathered from published data. Logic cores were simulated with a Rent exponent of $0.6$,
while memory cores used a value of $0.4$, \changed{and GPUs were simulated with a Rent exponent of $0.5$} \cite{christie_interpretation_2000}.
\changed{Each block was simulated separately to determine the number and
pitch of metal levels required for routing and the total power consumption of each block.
The pitch and number of metal layers used for the overall design were then set by the block which required
the greatest number of wiring tiers (typically the CPU core). This information, along with the geometry and power requirements
of each block was then used by the thermal module to determine the maximum temperature in the system.}

\begin{table}
	\centering
	%\begin{tabular}{|c|c|c|c|c|c|}
	\caption{Comparison to actual data }
	\begin{threeparttable}
	\begin{tabular}{cccccl}
		\hline
						&			&	\multicolumn{2}{c}{Signal Wire Tiers}								&	TDP		&	\multicolumn{1}{c}{Predicted} \\
		Processor		&	Node	&	Actual											&	Predicted		&	(W)		&	\multicolumn{1}{c}{Power (W)} \\
		\hline \hline
		E6850			&	65nm	&	8												&	8				&	65		&	60.27 \changed{(-7.3\%)}	\\
		E8600			&	45nm	&	\changed{8}\tnote{2}							&	8				&	65		&	63.62 \changed{(-2.1\%)}	\\
		i7 880			&	45nm	&	\changed{8}\tnote{2}							&	7				&	95		&	105.56 \changed{(11.1\%)}	\\
		i7 680\tnote{1}	&	32nm	&	\changed{8}\tnote{2}							&	6				&	73		&	52.74 \changed{(-27.8\%)}	\\
		i7 2700k		&	32nm	&	\changed{8}\tnote{2}							&	8				&	95		&	91.80 \changed{(-3.3\%)}	\\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item[1] Multi-chip package with 32nm CPU die and 45nm gpu/support die.
		\item[2] Design has one additional global metal layer for power distribution.
	\end{tablenotes}
	\end{threeparttable}
	\label{t-validation}	
\end{table}

The expected wire pitch, number of metal layers, power consumption, and
maximum junction temperature generated by the virtual platform have been compared in \cref{t-validation}. 
\changed{With the exception of the Core i7 680 test case, the virtual platform shows reasonable agreement with the
published data for these processors. The 45nm and 32nm test cases all have a total of 9 metal layers, but in all cases
the wires on the top layer are sized very large and used for power and clock delivery, rather than signal routing.
Since the interconnect estimation
module is used estimate the size and pitch of signal wires only, the top metal layer in these designs is not counted
for the purpose of signal wire pitch validation. Instead, the upper wire tier is modeled in the power delivery simulation
module of the virtual platform, but without data on the
power noise margin and the number of power and ground pads used in these designs, detailed validation of the power
delivery module is not possible for these test cases.
In addition to the total number of metal levels, the simulator
can accurately predict the wire pitch in each routing tier, as shown in \cref{f-sb-wire-pitch}.}

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb_wire_pitch_2.png}
	\caption{
		Actual and expected wire pitch in a Core i7 2700k processor.
	}
	\label{f-sb-wire-pitch}
\end{figure}

\changed{The simulator underestimates both the number of wiring tiers
and the overall power consumption for the Core i7 680 test case; this error could be due to the fact that the
Core i7 680 is the only design implemented as a multi-chip module (MCM), with a 32nm CPU die 
integrated with a 45nm GPU die in the same package.}
\changed{The simulator currently lacks the ability to estimate the power required for inter-block communication in
heterogeneous systems. While this
lack will affect the power estimates for all test cases, it is likely that the power required for communication between the two
separate dice in the Core i7 680 is much larger than the power requirements for communication between GPUs and logic cores
integrated on the same die.}


\changed{While the worst-case error in these benchmarks is relatively high ($27.8\%$), the value of this simulation framework lies
in its ability to consider many different effects very rapidly (a design scenario can be evaluated in several
seconds on a consumer-grade laptop computer), enabling detailed investigation of the relative performance
of different technologies on a particular design. Overall, this level of accuracy is sufficient for the investigation
of power and performance trends in 2D and 3DICs.
In the subsequent sections the virtual platform is used in this manner
to investigate the impacts of material, technology, and packaging innovations on the power consumption
and performance of 2D and 3DICs.}

\section{2D: Impact of materials innovation} \label{s-2d-investigations}
One path towards increasing system performance is to achieve improvements in the wiring materials.
The permittivity of the interlayer dielectric (ILD) material has a strong effect on the parasitic
capacitance of the on-chip wires,
which in turn impacts the wire RC delay and power consumption. Additionally, decreasing the RC delay
reduces the need for power-hungry repeaters.

In order to investigate the potential of ultra low-k (ULK) ILD materials, 
a 32nm Sandy Bridge Core i7 was simulated with a range of different
ILD permittivities, ranging from 3.9 (Silicon Dioxide), all the way down to 1 (vacuum).
Figure \ref{f-2d-materials-num-levels-ild} shows that the number of metal layers required to fully route
the Sandy Bridge processing core can be reduced from 8 to 6 if the relative dielectric constant
of the ILD material can be brought below 1.3.
CPU power consumption scales roughly linearly with
ILD permittivity, as shown in \cref{f-2d-materials-power-consumption}, and significant power reductions
are possible with ULK materials. The power reduction comes from both a reduction in wire power,
as well as a reduction in the number and size of the repeaters needed to meet timing constraints.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb_metal_layers_ild_eps_sweep_2.png}
	\caption{
		Impact of interlayer dielectric (ILD) permittivity on the number of metal layers required to route the wires in a Sandy Bridge Core i7 2700k.
	}
	\label{f-2d-materials-num-levels-ild}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/total_power_sb2d_ild_permittivity_sweep_3.png}
	\caption{
		Impact of interlayer dielectric (ILD) permittivity on the power consumed by wires and repeaters in a Sandy Bridge Core i7 2700k core.
	}
	\label{f-2d-materials-power-consumption}
\end{figure}

As the critical dimensions of the smallest on-chip wires have decreased,
electromigration has become a reliability concern at advanced process nodes 
\cite{michael_electromigration_2003,tokei_reliability_2010}.
In order to address electromigration challenges at advanced process nodes, alternate materials may be required, 
potentially impacting signal performance, power consumption,
and the number of metal layers required to fully route a design \cite{adelmann_alternative_2014}. % Removed sankaran_exploring_2014
It is likely that only the lowest metal levels would use alternate materials \cite{oates-electromigration-2015}. 

To investigate the potential impact of alternate metals on routing,
a hypothetical 7nm Sandy Bridge CPU test case was constructed by scaling the gate pitch, 
minimum wire pitch, transistor size, and all other lengths in the 32nm Sandy Bridge
by a factor of 4.57X (32/7). Two 7nm test cases were considered: \textit{7nm A}, in which all wires are composed of an
alternate material, and \textit{7nm B}, in which only wires thinner than 25nm are replaced by the alternate material.
The bulk resistivity of the alternate wiring material 
in both the 32nm and 7nm test cases was swept from 10 $\Omega$nm (slightly lower than bulk Ag), to 60 $\Omega$nm 
(slightly higher than bulk W). For simplicity, the specularity and reflection
parameters of the alternate wiring material are not modified.
As can be seen in \cref{f-2d-materials-num-levels-rho-7nm}, higher resistivity metals can significantly
increase the number of metal levels required for signal routing, but this effect can
be mitigated by restricting the use of alternate metals to the lowest wiring tiers.
Since the greatest numbers of wires are routed in the lowest metal tiers, small changes to their dimensions
can have a large impact on the wiring stack.



\section{3D: Power reduction without exotic materials} \label{s-3d-investigations}

\subsection{Reducing power consumption}
Implementing a design in 3D can greatly reduce the average length of the on-chip interconnects,
leading to reductions in the average delay and power consumption of the signaling network \cite{joyner_impact_2001}.
In order to examine the impact of 3D integration on power consumption,
a single 18.5mm$^2$ Sandy Bridge processing core was simulated in different 3D configurations.
Unless otherwise noted, we assume a TSV aspect ratio of 20:1, and require that the TSVs use
less than 10\% of the total die area. Typically, 3DIC designs limit the TSV area to 1\% or less
to minimize the cannibalization of active area, but we have relaxed that limit here for illustrative purposes.
In order to examine the impacts of 3D integration, a single CPU core from a 32nm Sandy Bridge Core i7 2700k
is examined throughout this section.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb2d_32nm_vs_7nm_alt_metal_comparison.png}
	\caption{
		Impact of wire resistivity on the number of metal layers required to route the wires in a Sandy Bridge CPU. Three cases are
		considered: a) a 32nm Sandy Bridge core; b) 7nm A, a hypothetical
		7nm Sandy Bridge core; and c) 7nm B, in which only wires with width below 25nm are modified.
	}
	\label{f-2d-materials-num-levels-rho-7nm}
\end{figure}

We consider a 3D integration scenario in which logic gates and blocks can be placed on
any tier, and in which TSVs are used as point to point interconnects.
The core is assumed to be partitioned into $N$ equal pieces, which are then stacked vertically.
This configuration is considered in order to illustrate the ultimate limits of 3D integration.

Significant power savings can be obtained by moving to a 3D design, as shown in \cref{f-sb-power-vs-tiers-ild},
though the power reduction comes at the cost of increased areal power density, ultimately placing more
stress on the cooling solution. The design implications of the increased power density of 3DICs will be discussed further
in \cref{s-3D-thermal}. It is important to note that 3DICs reduce the power required by on-chip communication, fundamentally 
improving the overall
energy efficiency of the system, as can be seen in \cref{f-sb-comm-power-fraction}.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb3d_power_and_pdens2.png}
	\caption{Impact of block folding on power consumption and power density of a single 32nm Sandy Bridge core.}
	\label{f-sb-power-vs-tiers-ild}
\end{figure}

In order to fully route a 3DIC, space must be allocated on each tier for TSVs. Since TSVs consume space
that could otherwise be used for logic, it is desirable to minimize the fraction of chip area 
consumed by TSVs. TSV diameter can be reduced by either increasing the TSV aspect ratio or by die thinning.
Thicker logic tiers are attractive due to their higher mechanical stability, but they reduce the wire length advantages of 3DICs. 
TSVs are typically limited to diameters of 5-10${\mu}m$ and aspect ratios between 5-20:1 
\cite{lau_evolution_2011,zhang_within-tier_2013}.

The impact of die thickness on 3DIC power consumption is examined in \cref{f-sb-wire-power-thickness}.
In order to realize the greatest possible power reduction from 3D integration, the active layers should be
as thin as possible, to minimize the distance that signals must travel when they are routed between tiers.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb3d_comm_power_fraction_4.png}
	\caption{Fraction of power consumed for on-chip communication as a function of 3D configuration and operating frequency.}
	\label{f-sb-comm-power-fraction}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb3d-comm-power-vs-thickness_3.png}
	\caption{Impact of die thickness on power consumed by wires and repeaters in a single 32nm Sandy Bridge core implemented in 3D.}
	\label{f-sb-wire-power-thickness}
\end{figure}



\subsection{Power Delivery}
Power delivery in 3DICs is challenging, as a high performance 3DIC may have a significantly
higher areal power density than an equivalent 2D chip, while simultaneously having less space available for
routing power delivery resources. Additionally, power must be delivered to each tier
through TSVs, introducing additional parasitic resistance and inductance into the wiring network.

The number of power connections required by each tier is determined by the power draw and power density 
of the system,
which depends upon the dielectric properties and the substrate thickness (for 3DICs).
Both 2D and 3D systems benefit from the development of ULK interlayer dielectrics, and 
the use of thin substrates in 3D configurations can further reduce the demands on the power supply network.

In order to explore these effects, the Sandy Bridge test case was simulated with several substrate thicknesses and 
dielectric permittivities, in order to determine the number of power delivery pads or TSVs needed to reduce
the simultaneous switching noise to below 15\% of the nominal supply voltage. The results are presented in \cref{f-3d-psn-ild-tiers}.
For two-tier stacks only a slight increase in power TSVs is observed over the 2D case, but
the eight-tier implementation requires roughly an order of magnitude
more power connections than the 2D design. The ILD permittivity has a strong impact on the power delivery requirements,
as it has an outsized impact on the power consumption of the on-chip communication network. The thickness 
of the 3DIC logic tiers is not a limiting factor for 2-tier designs, but 4- and 8-tier designs can realize nearly as much benefit
from die thinning as from ULK dielectrics.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb3d_psn_vs_ild_and_tiers_vacuum_vs_sio2_3.png}
	\caption{
		Impact of interlayer dielectric material, substrate thickness, and degree of 3D 
		integration on power delivery TSV requirements in a hypothetical 3D Sandy Bridge CPU core.
		In the single-tier (2D) case the required number of power pads is displayed.
	}
	\label{f-3d-psn-ild-tiers}
\end{figure}

Another method to reduce power supply noise is to integrate decoupling capacitors onto the die
to compensate for the inductance of the power delivery network. While this practice can improve
power quality, it also sets up a tradeoff between utilizing die area for logic and power delivery.

In order to explore this tradeoff, the 32nm Sandy Bridge test case was simulated in several 2D and 3D configurations with
varying amounts of silicon area allocated for chip- or interposer-level 
decoupling capacitors. The power TSV diameter is assumed to be 10 ${\mu}m$ and the thickness of each die in the 3D stack is assumed to be
10 ${\mu}m$ \changed{to investigate the potential of extreme die thinning. While processing and assembling thinned wafers can be challenging,
alternate integration schemes in which wafers are bonded and subsequently thinned could enable the stacking of such thin layers
without the need for modified wafer handling processes \cite{patti-three-dimensional-2006}}. Alternately, monolithic 3DIC fabrication
techniques could enable designs with extremely small intertier distances \cite{vinet-monolithic-2014}.
As can be seen in \cref{f-sb-psn-decap-10um}, increasing the decoupling
capacitance can significantly reduce the number of pads or TSVs required for power delivery.
Small area allocations can likely be achieved
with on-chip decoupling capacitors, but integrating high decoupling capacitance densities could be challenging. 
Even with the use of decoupling capacitors, a lower bound on power TSV number is set by
the need to keep the current density carried by each TSV low enough to avoid electromigration effects.

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/sb3d_psn_tiers_and_decap__substrate_10um__tsvs_10um_3.png}
	\caption{
		Power TSV requirements for a single Sandy Bridge core as a function of 3D configuration and
		area allocated for decoupling capacitors.
		}
	\label{f-sb-psn-decap-10um}
\end{figure}




\subsection{Thermal management} \label{s-3D-thermal}
Thermal management is a key challenge for 3DICs.
While the total power dissipation of an IC is expected to decrease as the system is partitioned into increasing numbers of layers 
(as shown in \cref{f-sb-power-vs-tiers-ild}),
the areal power density will still increase as tiers are stacked atop one another,
leading to increased stress on the cooling system.

\begin{table}[tb]
	\centering
	\caption{Material parameters used for thermal simulation}
	\begin{tabular}{cccccl}
		\hline
%						&	Thermal \\
		Material		 & Thermal Cond. (W/mK) \\
		\hline
		\hline
		Silicon			&	149 \\
		Copper			&	400 \\
		Underfill		&	0.3 \\
		Microbumps		&	60	\\
		Silicon Dioxide	&	1.38 \\
		\hline
	\end{tabular}
	\label{t-thermal-materials}	
\end{table}

\begin{figure}[tb]
	\centering
	\includegraphics[width=3.5in]{Figures/SB3D_max_frequency__air_vs_water_cooling.png}
	\caption{
		Maximum clock frequency of 2D and 3D 32nm Sandy Bridge CPU cores limited to 90$^\circ$C under air cooling (blue)
		and microfluidic cooling (red).
		}
	\label{f-sb3d-freq-vs-tiers}
\end{figure}

In order to quantify the thermal impact of 3D stacking, the performance of a single 32nm Sandy Bridge CPU core is examined 
\changed{in both 2D and 3D configurations, and with both air-cooled and microfluidic heat sinks. 
In all cases the heat sinks are located on the back side of the top die
in the stack. The air-cooled heat sink has a heat transfer coefficient of $1.83$ W/cm$^2$K and the microfluidic heat sink
has a heat transfer coefficient of $4.63$ W/cm$^2$K, as reported in \cite{zhang-micropinfin-heat-sink-2013}. Boundaries internal
to the package are assigned a heat transfer coefficient of $0.005$ W/cm$^2$K. The
thermal conductivities used for the materials in the stack are presented in \cref{t-thermal-materials}.}

The CPU core was simulated in each configuration to find 
the maximum operating frequency which could be maintained while keeping the maximum temperature below 90$^\circ$C.
\changed{In order to focus on solely the thermal aspects of 3D stacking we assumed that the system was thermally-limited, and ignored other
factors which could limit the maximum operating frequency of the chip, such as clock distribution. Additionally,
the power consumed by the cooling solution is ignored in both cases, in order to isolate intrinsic die-level
effects from the details of the particular heat sink used.}





As can be seen in \cref{f-sb3d-freq-vs-tiers}, the maximum frequency decreases steadily as the CPU is folded across
more tiers, as the increased power density (\cref{f-sb-power-vs-tiers-ild}) of the
system increases the strain on the cooling system.
\changed{In all cases, the microfluidically-cooled cores run faster than the air-cooled cores.}




%% =================================
%% ========= CONCLUSION ============
%% =================================

\section{Conclusion}
A virtual integration platform for 2D and 3D IC pathfinding was developed and used to examine the impacts of 
advanced technologies on system performance. The virtual platform incorporates 
models for signal delivery, power supply noise, and thermal performance in 2D and 3D ICs,
and was validated against wire pitch and power consumption data for recent commercial microprocessors.
The impacts of low-k dielectrics, and 3D integration on the performance, power consumption,
and power delivery requirements of a 32nm processing core were examined, and a hypothetical 7nm version of the same processor
was simulated to determine the impact of alternate wire materials on the number of metal levels required for signal routing
at advanced process nodes. \changed{The results suggest that 3D systems should exhibit greater energy efficiency
than their 2D counterparts, though high performance 3D logic cores will require more aggressive cooling solutions to achieve
performance parity. Highly-stacked high performance 3DICs were shown to require very large numbers of TSVs for power delivery,
due in part to the increased parasitic TSV resistance and inductance introduced into the power delivery network, as well as the
increased power density of the 3D cores. The use of highly-thinned dice as well as low-k interlayer dielectrics were shown to
be an effective methods for reducing the overall power consumption of high performance 3DICs.}



\section*{Acknowledgments}
The authors gratefully acknowledge the support of the Semiconductor
Research Corporation Global Research Collaboration (task ID 2254.001),
as well as the SRC Educational Alliance Intel Foundation Fellowship.

%% Fiddle with this to manually align columns on last page
%\enlargethispage{-1.0in}


%% Include bibliography
\bibliographystyle{IEEEtran_local}

% Use this to manually decrease the effective page size to balance the bibliography columns.
%May not work in all IEEE modes.
%\IEEEtriggercmd{\enlargethispage{-1.0in}} 

%% Use this to break the references at a particular reference number (don't use with enlargethispage)
%\IEEEtriggeratref{26}

% Start the bibliography!
\bibliography{references}

\vspace{-0.3in}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figures/bio-will.png}}]%
{William Wahby}
(S13) received the B.S. and M.S. degrees in electrical and computer engineering from the
University of Illinois at Urbana Champaign in 2007 and 2009, respectively. He worked
as a Product Engineer at Intel until 2011, when he
joined the Ph.D. program in electrical
and computer engineering at the Georgia Institute
of Technology. In 2014 he received the SRC Education Alliance
Intel Foundation Fellowship.
His research interests include monolithic 3D integration, chip-scale optical interconnects, 
and novel memory devices.
\end{IEEEbiography}

\vspace{-0.3in}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figures/bio-li.png}}]%
{Li Zheng}
received the B.S. degree from Zhejiang
University, Hangzhou, China, in 2006, and the dual
M.S. degree in electrical and computer engineering
from Shanghai Jiao Tong University, Shanghai,
China, and the Georgia Institute of Technology,
Atlanta, GA, USA, in 2009, where he is currently
pursuing the Ph.D. degree in electrical and computer
engineering.
His current research interests include embedded
microfluidic cooling, power delivery modeling
and chip-to-chip signaling modeling for high performance
2.5D and 3D systems.
\end{IEEEbiography}


\vspace{-0.3in}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figures/bio-yang.png}}]%
{Yang Zhang}
(S’13) received the B.S. degree
in microelectronics and mathematics from Peking
University, Beijing, China, in 2012. He is currently
pursuing the Ph.D. degree in electrical engineering
with the Georgia Institute of Technology, Atlanta,
GA, USA.
\end{IEEEbiography}

\vspace{-0.3in}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Figures/bio-muhannad.png}}]%
{Muhannad S. Bakir}
(SM’12) received the B.E.E.
(summa cum laude) degree from Auburn University,
Auburn, AL, USA, in 1999, and the M.S. and Ph.D.
degrees in electrical and computer engineering from
the Georgia Institute of Technology (Georgia Tech),
Atlanta, GA, USA, in 2000 and 2003, respectively.
He is currently an Associate Professor and the
ON Semiconductor Junior Professor with the School
of Electrical and Computer Engineering at Georgia
Tech.
Dr. Bakir is an Editor of the IEEE TRANSACTIONS
ON ELECTRON DEVICES, an Associate Editor of the IEEE TRANSACTIONS
ON COMPONENTS, PACKAGING AND MANUFACTURING TECHNOLOGY, and
was a Guest Editor of the June 2011 special issue of the IEEE Journal
of Selected Topics in Quantum Electronics. He is also a member of the
International Technology Roadmap for Semiconductors’ technical working
group for assembly and packaging. He was a recipient of the 2013 Intel
Early Career Faculty Honor Award, the 2012 DARPA Young Faculty Award,
the 2011 IEEE CPMT Society Outstanding Young Engineer Award, and was
an Invited Participant in the 2012 National Academy of Engineering Frontiers
of Engineering Symposium. He was also a recipient of the Semiconductor
Research Corporation’s Inventor Recognition Awards in 2002, 2005, and 2009.
He and his research group have received 12 conference and student paper
awards, including one from the IEEE Custom Integrated Circuits Conference,
five from the IEEE Electronic Components and Technology Conference, and
three from the IEEE International Interconnect Technology Conference.
\end{IEEEbiography}



\vfill


\end{document}

